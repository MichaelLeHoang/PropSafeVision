{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44fedeeb-1c48-4923-b09d-b554814b72ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: transformers in /work/m4le/.local/lib/python3.11/site-packages (4.52.3)\n",
      "Requirement already satisfied: filelock in /opt/module-jupyter-gpu/2.0-cuda-11-8/lib/python3.11/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /work/m4le/.local/lib/python3.11/site-packages (from transformers) (0.32.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/module-jupyter-gpu/2.0-cuda-11-8/lib/python3.11/site-packages (from transformers) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/module-jupyter-gpu/2.0-cuda-11-8/lib/python3.11/site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/module-jupyter-gpu/2.0-cuda-11-8/lib/python3.11/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /work/m4le/.local/lib/python3.11/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /opt/module-jupyter-gpu/2.0-cuda-11-8/lib/python3.11/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /work/m4le/.local/lib/python3.11/site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/module-jupyter-gpu/2.0-cuda-11-8/lib/python3.11/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/module-jupyter-gpu/2.0-cuda-11-8/lib/python3.11/site-packages (from transformers) (4.66.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/module-jupyter-gpu/2.0-cuda-11-8/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2024.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/module-jupyter-gpu/2.0-cuda-11-8/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.9.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /work/m4le/.local/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/module-jupyter-gpu/2.0-cuda-11-8/lib/python3.11/site-packages (from requests->transformers) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/module-jupyter-gpu/2.0-cuda-11-8/lib/python3.11/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/module-jupyter-gpu/2.0-cuda-11-8/lib/python3.11/site-packages (from requests->transformers) (2.0.5)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/module-jupyter-gpu/2.0-cuda-11-8/lib/python3.11/site-packages (from requests->transformers) (2023.7.22)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0c51ad1-4203-4320-ab49-1c0629d0f899",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: transformers\n",
      "Version: 4.52.3\n",
      "Summary: State-of-the-art Machine Learning for JAX, PyTorch and TensorFlow\n",
      "Home-page: https://github.com/huggingface/transformers\n",
      "Author: The Hugging Face team (past and future) with the help of all our contributors (https://github.com/huggingface/transformers/graphs/contributors)\n",
      "Author-email: transformers@huggingface.co\n",
      "License: Apache 2.0 License\n",
      "Location: /work/m4le/.local/lib/python3.11/site-packages\n",
      "Requires: filelock, huggingface-hub, numpy, packaging, pyyaml, regex, requests, safetensors, tokenizers, tqdm\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "!pip show transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c3cb101e-285d-459a-a9e4-9bf677d8b8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torch.optim import AdamW          \n",
    "from transformers import (\n",
    "    AutoImageProcessor,\n",
    "    ViTForImageClassification,\n",
    "    get_linear_schedule_with_warmup,\n",
    ")\n",
    "from tqdm.auto import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1f410231-42e3-4c2e-8374-90419065cb82",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.backends.cudnn.benchmark = True # Tells CUDA's cuDNN library to auto-tune and pick the fastest convolution algorithms for the imgsize\n",
    "\n",
    "use_amp = True\n",
    "scaler = torch.amp.GradScaler('cuda') if use_amp else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9b2c0205-15ee-4d60-9fa3-50b4bad5b651",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created train (675 images), valid (145 images), and test (145 images) datasets for class 'Kitchen'.\n",
      "Created train (3143 images), valid (674 images), and test (674 images) datasets for class 'Exterior'.\n",
      "Created train (873 images), valid (187 images), and test (188 images) datasets for class 'Bedroom'.\n",
      "Created train (810 images), valid (174 images), and test (174 images) datasets for class 'Dinning'.\n",
      "Created train (891 images), valid (191 images), and test (191 images) datasets for class 'Livingroom'.\n",
      "Created train (424 images), valid (91 images), and test (91 images) datasets for class 'Bathroom'.\n"
     ]
    }
   ],
   "source": [
    "# Make slit dataset\n",
    "import os\n",
    "import shutil\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "dataset_dir = \"full/\"\n",
    "train_dir = \"train/\"\n",
    "valid_dir = \"valid/\"\n",
    "test_dir = \"test/\"\n",
    "\n",
    "train_ratio = 0.7\n",
    "valid_ratio = 0.15\n",
    "test_ratio = 0.15\n",
    "\n",
    "for dir_path in [train_dir, valid_dir, test_dir]:\n",
    "    os.makedirs(dir_path, exist_ok=True)\n",
    "\n",
    "classes = [d for d in os.listdir(dataset_dir) if os.path.isdir(os.path.join(dataset_dir, d)) and d != '.ipynb_checkpoints']\n",
    "\n",
    "for class_name in classes:\n",
    "    for dir_path in [train_dir, valid_dir, test_dir]:\n",
    "        os.makedirs(os.path.join(dir_path, class_name), exist_ok=True)\n",
    "\n",
    "    # get all images files in the fill directores\n",
    "    origin_class_path = os.path.join(dataset_dir, class_name)\n",
    "    images = [f for f in os.listdir(origin_class_path) if f.lower().endswith(('.jpg', '.jpeg', '.png', '.ppm', '.bmp', '.pgm', '.tif', '.tiff', '.webp'))]\n",
    "\n",
    "    # splitting images into train, valid, test\n",
    "    train_images, temp_images = train_test_split(images, train_size=train_ratio, random_state=42)\n",
    "    valid_images, test_images = train_test_split(temp_images, train_size=test_ratio/(valid_ratio + test_ratio), random_state=42)\n",
    "\n",
    "    # move images to respective directories\n",
    "    for img in train_images:\n",
    "        shutil.copy(os.path.join(dataset_dir, class_name, img), os.path.join(train_dir, class_name, img))\n",
    "    for img in valid_images:\n",
    "        shutil.copy(os.path.join(dataset_dir, class_name, img), os.path.join(valid_dir, class_name, img))\n",
    "    for img in test_images:\n",
    "        shutil.copy(os.path.join(dataset_dir, class_name, img), os.path.join(test_dir, class_name, img))\n",
    "\n",
    "    print(f\"Created train ({len(train_images)} images), valid ({len(valid_images)} images), and test ({len(test_images)} images) datasets for class '{class_name}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fbc794d6-1c77-414d-8714-1a32339cac80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare datasets and dataloaders\n",
    "train_dir = \"train/\"\n",
    "valid_dir = \"valid/\"\n",
    "test_dir = \"test/\"\n",
    "\n",
    "processor = AutoImageProcessor.from_pretrained(\"google/vit-base-patch16-224\", use_fast=True)\n",
    "size = processor.size[\"height\"]\n",
    "\n",
    "train_tfms = transforms.Compose([\n",
    "    transforms.Resize((size,size)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=processor.image_mean,\n",
    "                         std=processor.image_std),\n",
    "    # transforms.Lambda(lambda t: t.half()),\n",
    "])\n",
    "valid_tfms = transforms.Compose([\n",
    "    transforms.Resize((size,size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=processor.image_mean,\n",
    "                         std=processor.image_std),\n",
    "    # transforms.Lambda(lambda t: t.half()),\n",
    "])\n",
    "test_tfms = valid_tfms\n",
    "\n",
    "train_ds = datasets.ImageFolder(train_dir, transform=train_tfms)\n",
    "valid_ds = datasets.ImageFolder(valid_dir, transform=valid_tfms)\n",
    "test_ds = datasets.ImageFolder(test_dir, transform=test_tfms)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=24, shuffle=True, num_workers=0, pin_memory=True)\n",
    "valid_loader = DataLoader(valid_ds, batch_size=24, shuffle=False, num_workers=0, pin_memory=True)\n",
    "test_loader = DataLoader(test_ds, batch_size=24, shuffle=False, num_workers=0, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c375a5a8-6de4-4c4a-8285-be28b6164ab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([6]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([1000, 768]) in the checkpoint and torch.Size([6, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Initial model,optimizer, scheduler\n",
    "model = ViTForImageClassification.from_pretrained(\n",
    "    \"google/vit-base-patch16-224\",\n",
    "    num_labels=len(train_ds.classes),\n",
    "    id2label={i:lab for i, lab in enumerate(train_ds.classes)},\n",
    "    label2id={lab:i for i, lab in enumerate(train_ds.classes)},\n",
    "    ignore_mismatched_sizes=True       \n",
    ").to(device)\n",
    "\n",
    "model = model.float().to(device)\n",
    "\n",
    "optim = AdamW(model.parameters(), lr=5e-5, weight_decay=0.01)\n",
    "\n",
    "num_epochs = 5\n",
    "total_steps = num_epochs * len(train_loader)\n",
    "sched = get_linear_schedule_with_warmup(\n",
    "    optim,\n",
    "    num_warmup_steps=int(0.1 * total_steps),\n",
    "    num_training_steps=total_steps,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b3a650cf-4b81-43ec-b2c2-933a9464bfaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training loop\n",
    "def train_epoch():\n",
    "    model.train()\n",
    "    losses = []\n",
    "    loop = tqdm(train_loader, desc=\"Train\")\n",
    "    for imgs, labels in loop:\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "        optim.zero_grad() \n",
    "\n",
    "        # forward + backward,with optional AMP\n",
    "        with torch.amp.autocast(\"cuda\"):\n",
    "            outputs = model(pixel_values=imgs, labels=labels)\n",
    "            loss = outputs.loss\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optim)\n",
    "        scaler.update()\n",
    "        sched.step()\n",
    "        losses.append(loss.item())\n",
    "        loop.set_postfix(loss=sum(losses)/len(losses))\n",
    "        \n",
    "    return sum(losses)/ len(losses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "705ef84a-4d3f-4550-84e4-011ea4627a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation Loop\n",
    "@torch.no_grad()\n",
    "def eval_epoch(loader=valid_loader):\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    loop = tqdm(loader, desc=\"Eval\")\n",
    "\n",
    "    for imgs, labels in loop:\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "        with torch.amp.autocast(\"cuda\") if use_amp else torch.no_grad():\n",
    "            logits = model(pixel_values=imgs).logits\n",
    "        preds = logits.argmax(-1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "        loop.set_postfix(acc=correct/total)\n",
    "    return correct/total\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "974ea43b-ecbb-46c4-9687-3e37fa54a88a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Epoch 1/10 ===\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2dae6f73cdbb40e3a2fe9ea611bcb19c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/284 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbc31ef335834371bfffdfa02cfe1e3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval:   0%|          | 0/61 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.4281  |  Val acc: 0.9590\n",
      "\n",
      "=== Epoch 2/10 ===\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22ca085f5eae4e1bb74ebdb9f4de592c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/284 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a7cc200cc81434b9ba070fe5d7d1160",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval:   0%|          | 0/61 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.0852  |  Val acc: 0.9569\n",
      "\n",
      "=== Epoch 3/10 ===\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb1382399c42443595f3780930624c2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/284 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d98ce20ddcfd4a90abd3a7ec6e16daf4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval:   0%|          | 0/61 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.0277  |  Val acc: 0.9651\n",
      "\n",
      "=== Epoch 4/10 ===\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d88227b2002d44e7a0a6a044fb8b184c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/284 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6472ab4c8274f3796294abc090ae8da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval:   0%|          | 0/61 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.0156  |  Val acc: 0.9699\n",
      "\n",
      "=== Epoch 5/10 ===\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "612b3d9fe0fa40d0be719e63cef7cc99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/284 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b17298933e5d4e33b730847c7b519698",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval:   0%|          | 0/61 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.0113  |  Val acc: 0.9706\n",
      "\n",
      "=== Epoch 6/10 ===\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67fb013feb8240ac9d5dd602f3ebc51d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/284 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b0c2b5b900447559eb3dab3c4de01fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval:   0%|          | 0/61 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.0069  |  Val acc: 0.9706\n",
      "\n",
      "=== Epoch 7/10 ===\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3bac2b45355479fa3be1c1e30591bf7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/284 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a7fc20ba776430db027561e6b126ac4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval:   0%|          | 0/61 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.0075  |  Val acc: 0.9706\n",
      "\n",
      "=== Epoch 8/10 ===\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "829cbf4644884066812e20dbdb28fe6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/284 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66afd0641c384bc88157bf20b974ed57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval:   0%|          | 0/61 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.0070  |  Val acc: 0.9706\n",
      "\n",
      "=== Epoch 9/10 ===\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a06d42005ac4a49a1a0c7bdc4c55472",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/284 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83cc18687a874d3b96d4038e31e461ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval:   0%|          | 0/61 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.0068  |  Val acc: 0.9706\n",
      "\n",
      "=== Epoch 10/10 ===\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7b2950cb0e343cda295191f4903305e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/284 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c32f73eefb143cb8ff448a69b75bdbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval:   0%|          | 0/61 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.0066  |  Val acc: 0.9706\n"
     ]
    }
   ],
   "source": [
    "best_accuracy = 0.0\n",
    "num_epochs = 10\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    print(f\"\\n=== Epoch {epoch}/{num_epochs} ===\")\n",
    "    train_loss = train_epoch()\n",
    "    val_acc = eval_epoch()\n",
    "    print(f\"Train loss: {train_loss:.4f}  |  Val acc: {val_acc:.4f}\")\n",
    "\n",
    "    # if val_acc > best_accuracy:\n",
    "    #     best_acc = val_acc\n",
    "    #     model.save_pretrained(\"vit-house-best\")\n",
    "    #     processor.save_pretrained(\"vit-house-best\")\n",
    "    #     print(\" Saved new best model\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
